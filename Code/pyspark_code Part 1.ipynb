{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basic Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T17:17:41.267039-05:00",
     "start_time": "2018-12-13T17:17:41.246880Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T17:17:48.588174-05:00",
     "start_time": "2018-12-13T17:17:48.291249Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T17:18:04.342119-05:00",
     "start_time": "2018-12-13T17:17:57.642142Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import argparse\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as mplt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "from pyspark.sql.functions import year,month,dayofmonth\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry_convert\n",
    "import pycountry\n",
    "import re\n",
    "import argparse\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "import matplotlib.pyplot as mplt\n",
    "import matplotlib.ticker as mtick\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load row data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "inferschema='true').load('/public/ygong15/xerox/volume.csv')\n",
    "\n",
    "asset=sqlContext.read.format('com.databricks.spark.csv').options(header='true', sep = '\\t', nullValue='NULL',ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True,\n",
    "inferschema='true').load('/public/ygong15/xerox/AssetData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Volume data part \n",
    "##### extract useful info\n",
    "##### Volume Analysis Data for Tableau analysis in python_code_code Part 2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume=volume.withColumn('dim_asset_inscope', volume['dim_asset_inscope'].cast(IntegerType()))\n",
    "volume=volume.withColumn('dim_asset_isdeleted', volume['dim_asset_isdeleted'].cast(IntegerType()))\n",
    "volume=volume.withColumn('dim_acct_sk', volume['dim_acct_sk'].cast(IntegerType()))\n",
    "volume=volume.fillna({'dim_asset_inscope':0})\n",
    "volume=volume.fillna({'dim_asset_isdeleted':0})\n",
    "volume=volume.where(col(\"dim_acct_sk\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Find the client who are only inscope or outscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_1=volume[volume['dim_asset_inscope']==1]\n",
    "volume_0=volume[volume['dim_asset_inscope']==0]\n",
    "\n",
    "in_and_out_scope=volume.groupBy('dim_acct_sk','dim_asset_inscope').count().sort('dim_acct_sk')\n",
    "in_and_out_scope=in_and_out_scope.where(col(\"dim_acct_sk\").isNotNull())\n",
    "in_or_out_count=in_and_out_scope.groupBy('dim_acct_sk').count()\n",
    "only_in_or_out=in_or_out_count[in_or_out_count['count']==1]\n",
    "a=only_in_or_out.join(in_and_out_scope,['dim_acct_sk'])\n",
    "tmp=a.select('dim_acct_sk','dim_asset_inscope')\n",
    "client_outscope=tmp[tmp['dim_asset_inscope']==0]\n",
    "client_inscope=tmp[tmp['dim_asset_inscope']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "client_inscope_list=client_inscope.select('dim_acct_sk').rdd.flatMap(lambda x: x).collect()\n",
    "client_outscope_list=client_outscope.select('dim_acct_sk').rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Select clients which exist volume are out of scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_has_out_scope=volume.filter(~volume.dim_acct_sk.isin(client_inscope_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp=volume_has_out_scope.select('dim_acct_sk').rdd.flatMap(lambda x: x).collect()\n",
    "client_has_outscope_list=list(set(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### save as volume_has_out_scope.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "volume_has_out_scope.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/xye6/Xerox/volume_has_out_scope.csv\",header = 'true')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Asset data part \n",
    "### (impute missing values)\n",
    "\n",
    "###### maxppm1108.csv \n",
    "###### Asset Analysis Data (for tableau analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check null value \n",
    "def is_missing (df):\n",
    "    # df type : spark data frame\n",
    "    df_columns = [i for i in df.columns]\n",
    "    total_missing_cnt = 0  \n",
    "    missing_value = []\n",
    "    for i in df_columns:\n",
    "        missing_value.append(df.filter(df[i].isNull()).count())\n",
    "        #print ('column ' + str(i) + ' has ' + str(missing_cnt) + ' missing values')\n",
    "    total_missing_cnt = reduce(lambda x,y: x+y, missing_value)\n",
    "    print  ('total number of missing cells over entire dataframe is ' + str(total_missing_cnt))\n",
    "    return dict(zip(df_columns,missing_value))\n",
    "# lower string letters\n",
    "lowercase_func = F.udf(lambda x : x.lower() if x is not None else x,types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create pyspark instance \n",
    "spark = SparkSession.builder.appName('My App').config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
    "# read as spark dataframe \n",
    "asset = spark.read.csv(path='/public/ygong15/xerox/AssetData.csv', header = True, sep='\\t', inferSchema=True, nullValue='NULL',\n",
    "                       ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True)\n",
    "asset = asset.drop('dim_asset_id','dim_model_manf_is_xerox_vs_nonxerox','dim_model_is_color_vs_mono','dim_last_status_date')\n",
    "# change dim_modified_dt_sk to timestamp\n",
    "asset = asset.withColumn('dim_modified_dt_sk', \n",
    "                   to_date(unix_timestamp(col('dim_modified_dt_sk'), 'yyyyMMdd').cast(\"timestamp\")))\n",
    "# drop the rows that the entire cells are null \n",
    "asset2 = asset.dropna(how='all')\n",
    "asset2 = asset2.filter(asset2.dim_model_nm.isNotNull())\n",
    "asset2 = asset2.withColumn('dim_model_nm', lowercase_func(asset2.dim_model_nm))\n",
    "\n",
    "\n",
    "## Split into two dataframes\n",
    "asset_month = asset2.filter(asset2.dim_asset_isdeleted == 0).\\\n",
    "select('dim_modified_dt_sk','dim_asset_sk','dim_acct_sk','dim_asset_inscope')\n",
    "## nature dataframe \n",
    "asset_nature = asset2.select('dim_asset_sk','dim_acct_sk','dim_model_nm','dim_model_manf_nm','dim_model_is_color',\n",
    "                             'dim_model_ppm','dim_model_manf_is_xerox','dim_loc_country','dim_loc_state',\n",
    "                             'dim_loc_city','dim_loc_region_name','dim_loc_iso_country_name')\n",
    "asset_nature = asset_nature.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### impute ppm part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "asset_nature = asset_nature.select('*',F.when(asset_nature.dim_model_ppm.isNull(),0).otherwise(asset2.dim_model_ppm).alias('ppm'))\n",
    "asset_nature = asset_nature.drop('dim_model_ppm')\n",
    "model_nm_df = asset_nature.filter(asset_nature.ppm != 0).select('dim_model_nm','ppm')\n",
    "model_nm_df = model_nm_df.dropDuplicates()\n",
    "model_nm_df2 = model_nm_df.groupBy('dim_model_nm').count()\n",
    "##multi-ppm \n",
    "multi_ppm = model_nm_df2.filter(model_nm_df2['count'] > 1).orderBy('count', ascending = False)\n",
    "multi_ppm_model = multi_ppm.select('dim_model_nm').rdd.map(lambda x : x[0]).collect()\n",
    "len(multi_ppm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## extract model with mono ppm \n",
    "mono_ppm_model = model_nm_df2.filter(model_nm_df2['count'] == 1).select('dim_model_nm').rdd.map(lambda x : x[0]).collect()\n",
    "mono_ppmDF = model_nm_df.filter(model_nm_df.dim_model_nm.isin(mono_ppm_model))\n",
    "print('num of rows = {i}'.format(i = mono_ppmDF.count()))\n",
    "## mono_ppm into a dictionary \n",
    "k_mono = mono_ppmDF.select('dim_model_nm').rdd.map(lambda x : x[0]).collect()\n",
    "v_mono = mono_ppmDF.select('ppm').rdd.map(lambda x : x[0]).collect()\n",
    "mono_ppm_dict = dict(zip(k_mono,v_mono)) \n",
    "print len(mono_ppm_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## extract multi ppm by manufactuer \n",
    "df_manf = asset_nature.filter(asset_nature.ppm != 0).select('dim_model_nm','ppm','dim_model_manf_nm')\n",
    "df_manf = df_manf.dropDuplicates()\n",
    "multi_modelDF = df_manf[df_manf.dim_model_nm.isin(multi_ppm_model)] \n",
    "model_manf_join = multi_modelDF.withColumn('model_manf',F.concat_ws('<>',multi_modelDF.dim_model_nm,multi_modelDF.dim_model_manf_nm))\n",
    "\n",
    "## multi_ppm into dict with --- max ppm\n",
    "model_manf_maxppm = model_manf_join.groupBy('model_manf').agg(F.max('ppm').alias('max_ppm'))\n",
    "k_mm =model_manf_maxppm.select('model_manf').rdd.map(lambda x : x[0]).collect()\n",
    "v_mm = model_manf_maxppm.select('max_ppm').rdd.map(lambda x : x[0]).collect()\n",
    "model_manf_dict = dict(zip(k_mm,v_mm))\n",
    "len(model_manf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ppm_mono_impute = F.udf(lambda x : mono_ppm_dict[x] if x in mono_ppm_dict.keys() else 0, types.IntegerType())\n",
    "ppm_multi_impute = F.udf(lambda x : model_manf_dict[x] if x in model_manf_dict.keys() else 0, types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assetppm = asset_nature.withColumn('model_manf',F.concat_ws('<>',asset_nature.dim_model_nm,asset_nature.dim_model_manf_nm))\n",
    "ppm_exist_df = assetppm.filter(assetppm.ppm != 0)\n",
    "ppm_zero_df = assetppm.filter(assetppm.ppm == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ppm_zero_df = ppm_zero_df.withColumn('ppm2', ppm_mono_impute(ppm_zero_df.dim_model_nm))\n",
    "ppm_zero_df = ppm_zero_df.withColumn('ppm3', F.when(ppm_zero_df.ppm2 ==0, ppm_multi_impute(ppm_zero_df.model_manf)).otherwise(0))\n",
    "ppm_zero_df = ppm_zero_df.withColumnRenamed('ppm3', 'dim_model_ppm')\n",
    "ppm_zero_df = ppm_zero_df.drop('ppm2','ppm','model_manf')\n",
    "ppm_exist_df = ppm_exist_df.drop('model_manf')\n",
    "ppm_exist_df = ppm_exist_df.withColumnRenamed('ppm', 'dim_model_ppm')\n",
    "df_concat = ppm_exist_df.union(ppm_zero_df)\n",
    "# df_concat.count() # 9165374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Impute location part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "country_iso3_dict = {u'BRB':'BRB',\n",
    " u'NDL':'NLD',\n",
    " u'ABW':'ABW',\n",
    " u'URY':'URY',\n",
    " u'BOT':'BWA',\n",
    " u'INACTIVE':None,\n",
    " u'LCA':'LCA',\n",
    " u'HRV':'HRV',\n",
    " u'ROM':'ROU',\n",
    " u'CG': 'COG',\n",
    " u'LI':'LIE',\n",
    " u'SIN': 'SGP',\n",
    " u'Outside North America': None,\n",
    " u'SLO': 'SVN',\n",
    " u'LES':'LSO',\n",
    " u'GB': 'GBR',\n",
    " u'SYC':'SYC',\n",
    " u'MRI':'MUS',\n",
    " u'IRQ':'IRQ',\n",
    " u'United Arab Emirates - Do Not Use':'ARE',\n",
    " u'CUW':'CUW',\n",
    " u'DEN':'DNK',\n",
    " u'NIC':'NIC',\n",
    " u'US':'USA',\n",
    " u'GGY':'GGY',\n",
    " u'LBA':'LBY',\n",
    " u'BIH':'BIH',\n",
    " u'france':'FRA',\n",
    " u'AL':'ALB',\n",
    " u'MWI':'MWI',\n",
    " u'UNKNOWN':None,\n",
    " u'ZZZDoNotUse_UK':'GBR',\n",
    " u'CH':'CHE',\n",
    " u'SWZ1':'SWZ',\n",
    " u'CIV':'CIV',\n",
    " u'MON':'MNG',\n",
    " u'ANGOLA':'AGO',\n",
    " u'GER':'DEU',\n",
    " u'UK':'GBR',\n",
    " u'UAE':'ARE',\n",
    " u'LS':'LSO',\n",
    " u'GD':'GRD',\n",
    " u'XXK':'XKX',\n",
    " u'CRO':'HRV',\n",
    " u'\\x8aLTU':'LTU',\n",
    " u'BES':'BES',\n",
    " u'GAB':'GAB',\n",
    " u'BE':'BEL',\n",
    " u'Dubai':'ARE',\n",
    " u'Praag':'CZE',\n",
    " u'PORTUGAL':'PRT',\n",
    " u'MKD':'MKD',\n",
    " u'ZAM':'ZMB',\n",
    " u'BSD':'BHS',\n",
    " u'TZA':'TZA',\n",
    " u'BB':'BRB',\n",
    " u'JEY':'JEY',\n",
    " u'Mauritius':'MUS',\n",
    " u'United Kingdom':'GBR',\n",
    " u'Netherlands':'NLD'}\n",
    "\n",
    "\n",
    "def convert_cty(iso3):\n",
    "    c = pycountry_convert.country_alpha2_to_continent_code(pycountry_convert.country_alpha3_to_country_alpha2(iso3))\n",
    "    return pycountry_convert.convert_continent_code_to_continent_name(c)\n",
    "\n",
    "iso_imupte = F.udf(lambda x : country_iso3_dict[x] if x in country_iso3_dict.keys() else x,types.StringType())\n",
    "region_impute = F.udf(lambda x : convert_cty(x) if x != None and x != 'XKX' else x, types.StringType())\n",
    "\n",
    "asset_iso = df_concat.select('*',\n",
    "                         F.when(df_concat.dim_loc_iso_country_name.isNull(), iso_imupte(df_concat.dim_loc_country))\\\n",
    "                          .otherwise(df_concat.dim_loc_iso_country_name).alias('country_iso3'))\n",
    "\n",
    "asset4 = asset_iso.drop('dim_loc_iso_country_name','dim_loc_country')\n",
    "asset4 = asset4.withColumn('region', F.when(asset4.country_iso3 =='XKX', 'Europe').otherwise(region_impute(asset4.country_iso3)))\n",
    "\n",
    "usa_state = asset4.filter(asset4.country_iso3 == 'USA').select('dim_loc_state').distinct().rdd.map(lambda x : x[0]).collect()\n",
    "impute_us_state_dict = {u'Hawaii':'HI', u'Texas':'TX',u'Pennsylvania':'PA',u'Illinois':'IL',u'Georgia':'GA',u'California':'CA'}\n",
    "impute_state_us= F.udf(lambda x :impute_us_state_dict[x] if x in impute_us_state_dict.keys() else x, types.StringType())\n",
    "asset4 = asset4.select('*',F.when(asset4.country_iso3 == 'USA', impute_state_us(asset4.dim_loc_state))\n",
    "                                  .otherwise(asset4.dim_loc_state).alias('state')) \n",
    "asset5 = asset4.drop('dim_loc_state','dim_loc_region_name')\n",
    "# asset5.count() # 9165374\n",
    "#asset5.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/ygong15/xerox/asset_natureV.1107\",header = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### impute with maxppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = asset5\n",
    "df = df.withColumn('model_manf',F.concat_ws('<>',df.dim_model_nm,df.dim_model_manf_nm))\n",
    "ppm_impute = F.udf(lambda x : model_manf_dict[x] if x in model_manf_dict.keys() else 0, types.IntegerType())\n",
    "df2 = df.withColumn('max_ppm', F.when(df.model_manf.isin(model_manf_dict.keys()),ppm_impute(df.model_manf))\\\n",
    "              .otherwise(df.dim_model_ppm))\n",
    "df2 = df2.drop('model_manf')\n",
    "## Save to final version of maxppm1108\n",
    "df2.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/ygong15/xerox/maxppm1108\",header = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### for Tableau Asset data\n",
    "##### Asset Over Month _Tableau Asset Analysis Data (for tableau analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## left join asset_month and asset_nature into one dataframe \n",
    "asset_nature = df2\n",
    "cond = [asset_month.dim_asset_sk == asset_nature.asset_id, asset_month.dim_acct_sk == asset_nature.account_id]\n",
    "left_joinDF = asset_month.join(asset_nature, \n",
    "                               cond,\n",
    "                               how='left')\n",
    "left_joinDF = left_joinDF.drop('asset_id','account_id')\n",
    "# left_joinDF.count() # 88430221\n",
    "\n",
    "## Tableau Data for asset over month \n",
    "new_assetmonth = left_joinDF\n",
    "scope = new_assetmonth.groupBy('month','dim_asset_inscope','dim_model_manf_nm','region','max_ppm')\\\n",
    ".agg(F.countDistinct('dim_asset_sk').alias('count'))\n",
    "# scope.count() #104228\n",
    "s =scope.toPandas()\n",
    "s.to_csv('/scratch/ygong15/xerox/asset_overall_1118.csv', header=True, index=False)\n",
    "\n",
    "## Tableau Data for Client over Month \n",
    "new_assetmonth = left_joinDF\n",
    "client = new_assetmonth.filter(new_assetmonth.max_ppm > 0).groupBy('month')\\\n",
    ".agg(F.countDistinct('dim_acct_sk').alias('count')).orderBy('month')\n",
    "c =client.toPandas()\n",
    "\n",
    "a = new_assetmonth.filter(new_assetmonth.max_ppm > 0).groupBy('month')\\\n",
    ".agg(F.countDistinct('dim_asset_sk').alias('asset_cnt')).orderBy('month')\n",
    "a_pd =a.toPandas()\n",
    "c_df = pd.merge(c, a_pd, how='left', on='month')\n",
    "c_df.to_csv('/scratch/ygong15/xerox/account_overtime.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Final step for getting final processed dataset \n",
    "### Final processed data will generate in part 2\n",
    "\n",
    "\n",
    "###### 1. volume_with_ppm_11_11.csv\n",
    "###### 2. inscope_11_11.csv and outscope_11_11.csv\n",
    "\n",
    "###### 3. asset_for_type.csv \n",
    "###### 4. black_outscope.csv / color_outscope.csv / black_inscope.csv / color_inscope.csv (csv for Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### load data got above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_has_out_scope = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "inferschema='true').load('/public/xye6/Xerox/volume_has_out_scope/volume_has_out_scope.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "impute_data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "inferschema='true').load('/public/ygong15/xerox/maxppm1108.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Join two table, Volume data get the ppm column (save as volume_with_ppm_11_11.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_with_maxppm=volume_has_out_scope.join(impute_data,['dim_asset_sk','dim_acct_sk'], how='left')\n",
    "\n",
    "volume_with_maxppm=volume_with_maxppm.withColumn(\"max_ppm\", volume_with_maxppm[\"max_ppm\"].cast(\"long\"))\n",
    "volume_with_maxppm=volume_with_maxppm.withColumn(\"dim_model_ppm\", volume_with_maxppm[\"dim_model_ppm\"].cast(\"long\"))\n",
    "\n",
    "volume_with_maxppm=volume_with_maxppm.withColumn(\"month_volume_by_maxppm\",volume_with_maxppm[\"max_ppm\"]*60*24*30)\n",
    "volume_with_maxppm=volume_with_maxppm.withColumn(\"month_volume_by_ppm\",volume_with_maxppm[\"dim_model_ppm\"]*60*24*30)\n",
    "\n",
    "volume_with_maxppm=volume_with_maxppm.withColumn(\"uti_maxppm\",volume_with_maxppm[\"Volume\"]/volume_with_maxppm[\"month_volume_by_maxppm\"])\n",
    "volume_with_maxppm=volume_with_maxppm.withColumn(\"uti_normalppm\",volume_with_maxppm[\"Volume\"]/volume_with_maxppm[\"month_volume_by_ppm\"])\n",
    "\n",
    "tmp=volume_with_maxppm.withColumnRenamed('dim_meter_types_sk','meter_type')\n",
    "tmp=tmp.withColumnRenamed('dim_asset_inscope','inscope')\n",
    "volume_with_maxppm=tmp.withColumnRenamed('dim_asset_isdeleted','isdeleted')\n",
    "\n",
    "\n",
    "targetDf = volume_with_maxppm.withColumn(\"Volume\", when((volume_with_maxppm.uti_maxppm >1.0) | \n",
    "                                                        (volume_with_maxppm.uti_maxppm < 0.0) | (volume_with_maxppm.uti_maxppm.isNull()), 0).otherwise(volume_with_maxppm.Volume))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#targetDf.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/xye6/Xerox/volume_with_ppm_11_11\",header = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get inscope volume table and outscope volume table ( inscope_11_11.csv and outscope_11_11.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# volume_11_11 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "# inferschema='true').load('/public/xye6/Xerox/volume_with_ppm_11_11/volume_11_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_11_11=targetDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "join_with_type=volume_11_11.select('dim_asset_sk','dim_acct_sk','meter_type','dim_invoice_dt_sk','Volume','inscope','isdeleted')\n",
    "\n",
    "join_with_type=join_with_type.withColumn(\"dim_invoice_dt_sk\", join_with_type[\"dim_invoice_dt_sk\"].cast(StringType()))\n",
    "\n",
    "join_with_type = join_with_type.withColumn('dim_invoice_dt_sk', \n",
    "                   to_date(unix_timestamp(col('dim_invoice_dt_sk'), 'yyyyMMdd').cast(\"timestamp\")))\n",
    "\n",
    "tmp=join_with_type.withColumn(\"month\",month(\"dim_invoice_dt_sk\"))\n",
    "join_with_type=tmp.withColumn(\"day\",dayofmonth(\"dim_invoice_dt_sk\"))\n",
    "\n",
    "join_with_type=join_with_type.withColumnRenamed('meter_type','dim_meter_types_sk')\n",
    "join_with_type=join_with_type.withColumnRenamed('inscope','dim_asset_inscope')\n",
    "join_with_type=join_with_type.withColumnRenamed('isdeleted','dim_asset_isdeleted')\n",
    "\n",
    "New_vol = join_with_type.select([\n",
    "    col('dim_asset_sk'),\n",
    "    col('dim_acct_sk'),\n",
    "    col('dim_asset_inscope'),\n",
    "    col('Volume'),\n",
    "    col('month'),\n",
    "    col('dim_meter_types_sk').alias('meter')\n",
    "])\n",
    "\n",
    "vol_inscope=New_vol.filter(New_vol.dim_asset_inscope==1)\n",
    "vol_outscope=New_vol.filter(New_vol.dim_asset_inscope==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#vol_inscope.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/xye6/Xerox/inscope_11_11.csv\",header=\"true\")\n",
    "#vol_outscope.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/xye6/Xerox/outscope_11_11.csv\",header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get asset_for_type.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "New_ass = asset.select([\n",
    "    col('dim_asset_sk').alias('asset_sk'), \n",
    "    col('dim_acct_sk').alias('acct_sk'), \n",
    "    col('dim_model_nm').alias('model_nm'),\n",
    "   # col('dim_asset_inscope'),\n",
    "    col('dim_modified_dt_sk')])\n",
    "\n",
    "New_ass = New_ass.withColumn('dim_modified_dt_sk', to_date(unix_timestamp(col('dim_modified_dt_sk'), 'yyyyMMdd').cast(\"timestamp\")))\n",
    "New_ass=New_ass.withColumn(\"Ass_month\",month(\"dim_modified_dt_sk\")).drop('dim_modified_dt_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#New_ass.coalesce(1).write.option(\"inferSchema\",\"true\").csv(\"/public/xye6/Xerox/asset_for_type.csv\",header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Type rule function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     15,
     47
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _join(New_ass, New_vol):\n",
    "    \n",
    "    final_table = New_vol.join(New_ass, \\\n",
    "                               (New_ass.asset_sk == New_vol.dim_asset_sk) & (New_ass.acct_sk == New_vol.dim_acct_sk) & (New_ass.Ass_month == New_vol.month), 'left').drop('asset_sk', 'acct_sk', 'Ass_month')\n",
    "\n",
    "    # The ones with ColorQube\n",
    "    \n",
    "    cq = final_table.where(col('model_nm').like('%ColorQube%'))\n",
    "    # The ones without ColorQube but with color\n",
    "    co = final_table.where((col('model_nm').like('%Color%')) & (~ (col('model_nm').like('%ColorQube%'))))\n",
    "    # All the others\n",
    "    bo = final_table.where((~(col('model_nm').like('%Color%'))) & (~ (col('model_nm').like('%ColorQube%'))))\n",
    "    \n",
    "    return cq, co, bo\n",
    "\n",
    "def monthly(mon, cq, co, bo):\n",
    "# ColorQube\n",
    "    cq_mon = cq.filter(cq['month'] == mon)\n",
    "    ### Black and white\n",
    "    cq_mon_bw = cq_mon.filter((cq_mon['meter'] == 35) | (cq_mon['meter'] == 32))\n",
    "    cq_mon_bw = cq_mon_bw.groupby('dim_asset_sk', 'dim_acct_sk', 'month').agg(F.sum('Volume').alias('BlackVolume'))\n",
    "    ### Color\n",
    "    cq_mon_col = cq_mon.filter((cq_mon['meter'] == 30) | (cq_mon['meter'] == 33) | (cq_mon['meter'] == 31) | (cq_mon['meter'] == 34))\n",
    "    cq_mon_col = cq_mon_col.groupby('dim_asset_sk', 'dim_acct_sk', 'month').agg(F.sum('Volume').alias('ColorVolume'))\n",
    "\n",
    "    # Color\n",
    "    ## Color\n",
    "    co_mon = co.filter(co['month'] == mon)\n",
    "    ### Black and white\n",
    "    co_mon_bw = co_mon.filter(co_mon['meter'] == 2)\n",
    "    co_mon_bw = co_mon_bw.groupby('dim_asset_sk', 'dim_acct_sk', 'month').agg(F.sum('Volume').alias('BlackVolume'))\n",
    "    ### Color\n",
    "    co_mon_col = co_mon.filter(co_mon['meter'] == 3)\n",
    "    co_mon_col = co_mon_col.groupby('dim_asset_sk', 'dim_acct_sk', 'month').agg(F.sum('Volume').alias('ColorVolume'))\n",
    "\n",
    "    # All the others\n",
    "    bo_mon = bo.filter(bo['month'] == mon)\n",
    "    ### Black and white\n",
    "    bo_mon_bw = bo_mon.filter(bo_mon['meter'] == 1)\n",
    "    bo_mon_bw = bo_mon_bw.groupby('dim_asset_sk', 'dim_acct_sk', 'month').agg(F.sum('Volume').alias('BlackVolume'))\n",
    "\n",
    "    # Concatenate\n",
    "    total_bw = cq_mon_bw.union(co_mon_bw).union(bo_mon_bw)\n",
    "    total_col = cq_mon_col.union(co_mon_col)\n",
    "    return total_bw, total_col\n",
    "\n",
    "def _get(New_ass, New_vol):\n",
    "    cq, co, bo = _join(New_ass, New_vol)\n",
    "    final_bw, final_col = monthly(1, cq, co, bo)\n",
    "    \n",
    "    for mon in range(2,13):\n",
    "        bw, col = monthly(mon, cq, co, bo)\n",
    "        final_bw=final_bw.union(bw)\n",
    "        final_col=final_col.union(col)\n",
    "        \n",
    "    return final_bw, final_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Process data in Type rule part (inscope_11_11.csv | outscope_11_11.csv | asset_for_type.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# vol_inscope = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "# inferschema='true').load('/public/xye6/Xerox/inscope_11_11/inscope_11_11.csv')\n",
    "\n",
    "# vol_outscope = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "# inferschema='true').load('/public/xye6/Xerox/outscope_11_11/outscope_11_11.csv')\n",
    "\n",
    "# asset_for_type = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "# inferschema='true').load('/public/xye6/Xerox/asset_for_type.csv')\n",
    "\n",
    "\n",
    "asset_for_type=New_ass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "black_in,color_in=_get(asset_for_type,vol_inscope)\n",
    "black_out,color_out=_get(asset_for_type,vol_outscope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get black_outscope.csv / color_outscope.csv / black_inscope.csv / color_inscope.csv  (csv for Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "black_outscope=black_out.toPandas()\n",
    "black_outscope.to_csv('/public/xye6/Xerox/black_outscope.csv', sep='\\t')\n",
    "\n",
    "color_outscope=color_out.toPandas()\n",
    "color_outscope.to_csv('/public/xye6/Xerox/color_outscope.csv', sep='\\t')\n",
    "\n",
    "black_inscope=black_in.toPandas()\n",
    "black_inscope.to_csv('/public/xye6/Xerox/black_inscope.csv', sep='\\t')\n",
    "\n",
    "color_inscope=color_in.toPandas()\n",
    "color_inscope.to_csv('/public/xye6/Xerox/color_inscope.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate volume_location.csv \n",
    "### for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## left join asset_month and asset_nature into one dataframe \n",
    "asset_nature = df2\n",
    "cond = [asset_month.dim_asset_sk == asset_nature.asset_id, asset_month.dim_acct_sk == asset_nature.account_id]\n",
    "left_joinDF = asset_month.join(asset_nature, \n",
    "                               cond,\n",
    "                               how='left')\n",
    "left_joinDF = left_joinDF.drop('asset_id','account_id')\n",
    "# left_joinDF.count() # 88430221\n",
    "\n",
    "## create Tableau Data \n",
    "new_assetmonth = left_joinDF\n",
    "scope = new_assetmonth.groupBy('month','dim_asset_inscope','dim_model_manf_nm','region','max_ppm')\\\n",
    ".agg(F.countDistinct('dim_asset_sk').alias('count'))\n",
    "# scope.count() #104228\n",
    "s =scope.toPandas()\n",
    "# s.to_csv('/scratch/ygong15/xerox/asset_overall_1118.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_with_maxppm = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "inferschema='true').load('/public/xye6/Xerox/volume_with_ppm_1107/volume_with_ppm_1107.csv')\n",
    "\n",
    "volume_with_maxppm.filter(volume_with_maxppm.dim_model_nm.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_locinfo=volume_with_maxppm.select('dim_asset_sk','dim_acct_sk','dim_model_nm','dim_model_manf_nm','dim_model_manf_is_xerox','dim_loc_city','country_iso3',\n",
    "                          'region','state','max_ppm','dim_model_ppm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_locinfo=volume_locinfo.dropDuplicates(['dim_asset_sk','dim_acct_sk','dim_model_nm','dim_model_manf_nm'])\n",
    "\n",
    "volume_location=volume_locinfo.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_location.to_csv('/public/xye6/Xerox/volume_location.csv', sep='\\t',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (intel+spark)",
   "language": "python",
   "name": "intel-python-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
